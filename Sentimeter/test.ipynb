{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the trained model from the file with a different extension\n",
    "with open(\"C:\\\\Users\\\\SATWIK M BADIGER\\\\Desktop\\\\projects\\\\ML\\\\Sentimeter\\\\naive_bayes_model.pkl\", 'rb') as file:\n",
    "    clf = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   channelId      videoId  \\\n",
      "0   UCJtUOos_MwJa_Ewii-R3cJA  WNrB1Q9Rry0   \n",
      "1   UCJtUOos_MwJa_Ewii-R3cJA  WNrB1Q9Rry0   \n",
      "2   UCJtUOos_MwJa_Ewii-R3cJA  WNrB1Q9Rry0   \n",
      "3   UCJtUOos_MwJa_Ewii-R3cJA  WNrB1Q9Rry0   \n",
      "4   UCJtUOos_MwJa_Ewii-R3cJA  WNrB1Q9Rry0   \n",
      "5   UCJtUOos_MwJa_Ewii-R3cJA  WNrB1Q9Rry0   \n",
      "6   UCJtUOos_MwJa_Ewii-R3cJA  WNrB1Q9Rry0   \n",
      "7   UCJtUOos_MwJa_Ewii-R3cJA  WNrB1Q9Rry0   \n",
      "8   UCJtUOos_MwJa_Ewii-R3cJA  WNrB1Q9Rry0   \n",
      "9   UCJtUOos_MwJa_Ewii-R3cJA  WNrB1Q9Rry0   \n",
      "10  UCJtUOos_MwJa_Ewii-R3cJA  WNrB1Q9Rry0   \n",
      "11  UCJtUOos_MwJa_Ewii-R3cJA  WNrB1Q9Rry0   \n",
      "12  UCJtUOos_MwJa_Ewii-R3cJA  WNrB1Q9Rry0   \n",
      "13  UCJtUOos_MwJa_Ewii-R3cJA  WNrB1Q9Rry0   \n",
      "14  UCJtUOos_MwJa_Ewii-R3cJA  WNrB1Q9Rry0   \n",
      "15  UCJtUOos_MwJa_Ewii-R3cJA  WNrB1Q9Rry0   \n",
      "16  UCJtUOos_MwJa_Ewii-R3cJA  WNrB1Q9Rry0   \n",
      "17  UCJtUOos_MwJa_Ewii-R3cJA  WNrB1Q9Rry0   \n",
      "18  UCJtUOos_MwJa_Ewii-R3cJA  WNrB1Q9Rry0   \n",
      "19  UCJtUOos_MwJa_Ewii-R3cJA  WNrB1Q9Rry0   \n",
      "20  UCJtUOos_MwJa_Ewii-R3cJA  WNrB1Q9Rry0   \n",
      "21  UCJtUOos_MwJa_Ewii-R3cJA  WNrB1Q9Rry0   \n",
      "22  UCJtUOos_MwJa_Ewii-R3cJA  WNrB1Q9Rry0   \n",
      "23  UCJtUOos_MwJa_Ewii-R3cJA  WNrB1Q9Rry0   \n",
      "24  UCJtUOos_MwJa_Ewii-R3cJA  WNrB1Q9Rry0   \n",
      "25  UCJtUOos_MwJa_Ewii-R3cJA  WNrB1Q9Rry0   \n",
      "26  UCJtUOos_MwJa_Ewii-R3cJA  WNrB1Q9Rry0   \n",
      "27  UCJtUOos_MwJa_Ewii-R3cJA  WNrB1Q9Rry0   \n",
      "28  UCJtUOos_MwJa_Ewii-R3cJA  WNrB1Q9Rry0   \n",
      "29  UCJtUOos_MwJa_Ewii-R3cJA  WNrB1Q9Rry0   \n",
      "30  UCJtUOos_MwJa_Ewii-R3cJA  WNrB1Q9Rry0   \n",
      "31  UCJtUOos_MwJa_Ewii-R3cJA  WNrB1Q9Rry0   \n",
      "32  UCJtUOos_MwJa_Ewii-R3cJA  WNrB1Q9Rry0   \n",
      "33  UCJtUOos_MwJa_Ewii-R3cJA  WNrB1Q9Rry0   \n",
      "34  UCJtUOos_MwJa_Ewii-R3cJA  WNrB1Q9Rry0   \n",
      "35  UCJtUOos_MwJa_Ewii-R3cJA  WNrB1Q9Rry0   \n",
      "36  UCJtUOos_MwJa_Ewii-R3cJA  WNrB1Q9Rry0   \n",
      "37  UCJtUOos_MwJa_Ewii-R3cJA  WNrB1Q9Rry0   \n",
      "38  UCJtUOos_MwJa_Ewii-R3cJA  WNrB1Q9Rry0   \n",
      "39  UCJtUOos_MwJa_Ewii-R3cJA  WNrB1Q9Rry0   \n",
      "40  UCJtUOos_MwJa_Ewii-R3cJA  WNrB1Q9Rry0   \n",
      "41  UCJtUOos_MwJa_Ewii-R3cJA  WNrB1Q9Rry0   \n",
      "42  UCJtUOos_MwJa_Ewii-R3cJA  WNrB1Q9Rry0   \n",
      "43  UCJtUOos_MwJa_Ewii-R3cJA  WNrB1Q9Rry0   \n",
      "44  UCJtUOos_MwJa_Ewii-R3cJA  WNrB1Q9Rry0   \n",
      "45  UCJtUOos_MwJa_Ewii-R3cJA  WNrB1Q9Rry0   \n",
      "46  UCJtUOos_MwJa_Ewii-R3cJA  WNrB1Q9Rry0   \n",
      "47  UCJtUOos_MwJa_Ewii-R3cJA  WNrB1Q9Rry0   \n",
      "48  UCJtUOos_MwJa_Ewii-R3cJA  WNrB1Q9Rry0   \n",
      "49  UCJtUOos_MwJa_Ewii-R3cJA  WNrB1Q9Rry0   \n",
      "\n",
      "                                          textDisplay  \\\n",
      "0   <a href=\"UCJtUOos_MwJa_Ewii-R3cJA/INVYZKnjF56F...   \n",
      "1   Hey Leila, thanks for all the tips! I am wonde...   \n",
      "2   All these options in insert are not coming in ...   \n",
      "3   I materials had code if i want both displayed ...   \n",
      "4   About time excel got with the picture. But fir...   \n",
      "5   Was this a phased release? My images are showi...   \n",
      "6   Wow!!<br>Thanks a lot for sharing these nice t...   \n",
      "7   This is super helpful and useful as well! Than...   \n",
      "8   @LeilaGharani , I have kind of a challenge for...   \n",
      "9   As always, great presentation üôÇ Works for me i...   \n",
      "10  Thanks for the class.  Can the same be done wi...   \n",
      "11    When will it be available to general 365 users?   \n",
      "12  Love that you can now insert pictures into cel...   \n",
      "13  Awesome üëè. I have a question. Can I create a f...   \n",
      "14  can you make videos how to run macros in prote...   \n",
      "15  Hi Leila, love the show! Thank you for all the...   \n",
      "16  In my Excel, I can find the Data &gt; Data Typ...   \n",
      "17  Dear Leila, Thank you for vid. It is amazing. ...   \n",
      "18                                              üáÆüá≥üëç‚ù§Ô∏è   \n",
      "19                           Great video!  Excellent!   \n",
      "20  Limitation if I have multiple columns in value...   \n",
      "21  Your videos are great, but you go to fast to l...   \n",
      "22                        wouwwwwwwwww .... amazing!!   \n",
      "23  This was brilliant! Kudos for putting in all t...   \n",
      "24                                                  üëç   \n",
      "25  Wow Awesome thanks for updating us for new and...   \n",
      "26  Hi, as you top student, surprise me with somet...   \n",
      "27  Leila  .hi . Im a grt fan of yours‚ù§Ô∏è<br>When u...   \n",
      "28                                  Amazing as usual.   \n",
      "29  AOA<br>copy a data from filtered cells, where ...   \n",
      "30                         She‚Äôs not hard to look at.   \n",
      "31                                               Cool   \n",
      "32                                         üíï Love it!   \n",
      "33  I am using HP Envy 15x360 2 in 1 laptop.When I...   \n",
      "34                                        Amazing ...   \n",
      "35                                                 ‚Ä¶.   \n",
      "36                             Great video. Thank you   \n",
      "37  Fantastic video!  Can‚Äôt wait to the view the o...   \n",
      "38              ŸÖŸÖŸÜŸàŸÜŸÖ ÿßÿ≤ ÿ®Ÿá ÿßÿ¥ÿ™ÿ±ÿßŸÉ ⁄Øÿ∞ÿßÿ¥ÿ™ŸÜ ÿßÿ∑ŸÑÿßÿπÿßÿ™ÿ™ŸàŸÜ   \n",
      "39  Hi Leila you are awesome ‚ù§and I have a problem...   \n",
      "40  hey Leila~,<br>Not sure how I got unSub~d, <br...   \n",
      "41  How do you make tutorials on features we don&#...   \n",
      "42  An amazing insight; I&#39;m not sure if i will...   \n",
      "43                                    Those salaries!   \n",
      "44  Thank you for this video.  I have traveling an...   \n",
      "45  I truly like all these ideas and suggestions f...   \n",
      "46  Hi. Im hoping if you can help pls. Having issu...   \n",
      "47  Hi Leila, that&#39;s amazing, I have an issue:...   \n",
      "48  This can be a life saver!üëèüèºüëèüèºüëèüèº My colleague j...   \n",
      "49  ok thanks so much on this explanation , as usu...   \n",
      "\n",
      "                                         textOriginal  \\\n",
      "0    Stay ahead with our Weekly Newsletter. Get th...   \n",
      "1   Hey Leila, thanks for all the tips! I am wonde...   \n",
      "2   All these options in insert are not coming in ...   \n",
      "3   I materials had code if i want both displayed ...   \n",
      "4   About time excel got with the picture. But fir...   \n",
      "5   Was this a phased release? My images are showi...   \n",
      "6   Wow!!\\nThanks a lot for sharing these nice tri...   \n",
      "7   This is super helpful and useful as well! Than...   \n",
      "8   @LeilaGharani , I have kind of a challenge for...   \n",
      "9   As always, great presentation üôÇ Works for me i...   \n",
      "10  Thanks for the class.  Can the same be done wi...   \n",
      "11    When will it be available to general 365 users?   \n",
      "12  Love that you can now insert pictures into cel...   \n",
      "13  Awesome üëè. I have a question. Can I create a f...   \n",
      "14  can you make videos how to run macros in prote...   \n",
      "15  Hi Leila, love the show! Thank you for all the...   \n",
      "16  In my Excel, I can find the Data > Data Types ...   \n",
      "17  Dear Leila, Thank you for vid. It is amazing. ...   \n",
      "18                                              üáÆüá≥üëç‚ù§Ô∏è   \n",
      "19                           Great video!  Excellent!   \n",
      "20  Limitation if I have multiple columns in value...   \n",
      "21  Your videos are great, but you go to fast to l...   \n",
      "22                        wouwwwwwwwww .... amazing!!   \n",
      "23  This was brilliant! Kudos for putting in all t...   \n",
      "24                                                  üëç   \n",
      "25  Wow Awesome thanks for updating us for new and...   \n",
      "26  Hi, as you top student, surprise me with somet...   \n",
      "27  Leila  .hi . Im a grt fan of yours‚ù§Ô∏è\\nWhen usi...   \n",
      "28                                  Amazing as usual.   \n",
      "29  AOA\\ncopy a data from filtered cells, where af...   \n",
      "30                         She‚Äôs not hard to look at.   \n",
      "31                                               Cool   \n",
      "32                                         üíï Love it!   \n",
      "33  I am using HP Envy 15x360 2 in 1 laptop.When I...   \n",
      "34                                        Amazing ...   \n",
      "35                                                 ‚Ä¶.   \n",
      "36                             Great video. Thank you   \n",
      "37  Fantastic video!  Can‚Äôt wait to the view the o...   \n",
      "38              ŸÖŸÖŸÜŸàŸÜŸÖ ÿßÿ≤ ÿ®Ÿá ÿßÿ¥ÿ™ÿ±ÿßŸÉ ⁄Øÿ∞ÿßÿ¥ÿ™ŸÜ ÿßÿ∑ŸÑÿßÿπÿßÿ™ÿ™ŸàŸÜ   \n",
      "39  Hi Leila you are awesome ‚ù§and I have a problem...   \n",
      "40  hey Leila~,\\nNot sure how I got unSub~d, \\nsur...   \n",
      "41  How do you make tutorials on features we don't...   \n",
      "42  An amazing insight; I'm not sure if i will be ...   \n",
      "43                                    Those salaries!   \n",
      "44  Thank you for this video.  I have traveling an...   \n",
      "45  I truly like all these ideas and suggestions f...   \n",
      "46  Hi. Im hoping if you can help pls. Having issu...   \n",
      "47  Hi Leila, that's amazing, I have an issue: the...   \n",
      "48  This can be a life saver!üëèüèºüëèüèºüëèüèº My colleague j...   \n",
      "49  ok thanks so much on this explanation , as usu...   \n",
      "\n",
      "            authorDisplayName  \\\n",
      "0               @LeilaGharani   \n",
      "1                   @Klarkooi   \n",
      "2              @adarshjha4774   \n",
      "3                  @truth5119   \n",
      "4             @michaeljarcher   \n",
      "5             @onewhitewolf13   \n",
      "6   @md.mahmudulalamsymon1216   \n",
      "7                 @iditenahui   \n",
      "8                 @siryoneyal   \n",
      "9                    @kn4golf   \n",
      "10               @walterf6763   \n",
      "11         @FutureCommentary1   \n",
      "12                   @m_stedt   \n",
      "13                    @wr3661   \n",
      "14          @SilverVillacacan   \n",
      "15                 @RayMedina   \n",
      "16                @gui.liraaa   \n",
      "17              @2000sunsunny   \n",
      "18         @prajwalshetty2047   \n",
      "19               @drewsoffice   \n",
      "20   @kashviyashvikimasti5389   \n",
      "21                 @dougc1753   \n",
      "22              @jesus.moreno   \n",
      "23                @TurnToData   \n",
      "24         @PoojaAdnani-wk6pp   \n",
      "25         @sarmadpirzada8485   \n",
      "26         @hamphrey.olendo65   \n",
      "27      @tanweerabbasabbas649   \n",
      "28           @user-xi8oh4ne4u   \n",
      "29           @fidahussain6430   \n",
      "30                 @mckavelli   \n",
      "31            @tamerhegab1513   \n",
      "32          @dianechapman9887   \n",
      "33               @arsakib8186   \n",
      "34           @joaquimcosta952   \n",
      "35          @ericssontest2332   \n",
      "36                  @AJ-et3vf   \n",
      "37            @johnwilson7845   \n",
      "38           @hadiasghari2297   \n",
      "39           @omidshirazi4166   \n",
      "40               @yachid_6516   \n",
      "41           @enocharthur4322   \n",
      "42            @FarhanMerchant   \n",
      "43           @JakeSmith-jy1kx   \n",
      "44                 @garys2187   \n",
      "45                 @Moker2012   \n",
      "46                @mrjaydp123   \n",
      "47           @user-gm5dc4mt6t   \n",
      "48                 @Zoe-dm1ch   \n",
      "49           @user-wx1ti7qo5z   \n",
      "\n",
      "                                authorProfileImageUrl  \\\n",
      "0   https://yt3.ggpht.com/ytc/AIdro_nRcy9yiYCsCEjQ...   \n",
      "1   https://yt3.ggpht.com/ytc/AIdro_losLLKp7Xn8LKI...   \n",
      "2   https://yt3.ggpht.com/ytc/AIdro_nJL8X1FJyPyihb...   \n",
      "3   https://yt3.ggpht.com/ytc/AIdro_laowbR60Lv85Da...   \n",
      "4   https://yt3.ggpht.com/ytc/AIdro_l_nbofPKC-RbZr...   \n",
      "5   https://yt3.ggpht.com/ytc/AIdro_nQzyCHZTH8fvmi...   \n",
      "6   https://yt3.ggpht.com/ytc/AIdro_mIEw5bfzDwWGIg...   \n",
      "7   https://yt3.ggpht.com/GLlhdYP4vVn3GJqleERmksCl...   \n",
      "8   https://yt3.ggpht.com/ytc/AIdro_l--pLvO-SZRpfZ...   \n",
      "9   https://yt3.ggpht.com/ytc/AIdro_l67gZiWw14bHIm...   \n",
      "10  https://yt3.ggpht.com/ytc/AIdro_ljh2eH5vgLDdly...   \n",
      "11  https://yt3.ggpht.com/ytc/AIdro_kmhVH0lFVZfX_B...   \n",
      "12  https://yt3.ggpht.com/ytc/AIdro_l9Jc9hsgwFNJvd...   \n",
      "13  https://yt3.ggpht.com/ytc/AIdro_l35hxf4KfE4_iK...   \n",
      "14  https://yt3.ggpht.com/1HwsR24n6ej8JQshresBUsgM...   \n",
      "15  https://yt3.ggpht.com/ytc/AIdro_ldQ6XUSf9VbSqi...   \n",
      "16  https://yt3.ggpht.com/ytc/AIdro_mhfOCVvHGYWso-...   \n",
      "17  https://yt3.ggpht.com/ytc/AIdro_mNKQOpxZX67dO1...   \n",
      "18  https://yt3.ggpht.com/ytc/AIdro_lsF8cUv3tlsC6Y...   \n",
      "19  https://yt3.ggpht.com/2KjJzUTRVGA3EuAPXCiUSE77...   \n",
      "20  https://yt3.ggpht.com/ytc/AIdro_kKZlTbnrgzCSGe...   \n",
      "21  https://yt3.ggpht.com/ytc/AIdro_nW0VQwF0ChmW6k...   \n",
      "22  https://yt3.ggpht.com/ytc/AIdro_k7WGCs8v4nxn1q...   \n",
      "23  https://yt3.ggpht.com/hjV4aIk_NbP0NDzkx_S7s_m3...   \n",
      "24  https://yt3.ggpht.com/ytc/AIdro_kQb66_r-tkBEn2...   \n",
      "25  https://yt3.ggpht.com/ytc/AIdro_lxZPHiLu25GiQZ...   \n",
      "26  https://yt3.ggpht.com/ytc/AIdro_kzBrYbpGciIHKU...   \n",
      "27  https://yt3.ggpht.com/ytc/AIdro_kVqQQOYc-BJuAq...   \n",
      "28  https://yt3.ggpht.com/ytc/AIdro_k18TQOefWzLa5S...   \n",
      "29  https://yt3.ggpht.com/ytc/AIdro_mM3p2Hkq2OCXkR...   \n",
      "30  https://yt3.ggpht.com/ytc/AIdro_k4RymunxApH7OH...   \n",
      "31  https://yt3.ggpht.com/ytc/AIdro_m89QLG5ZtdWDJX...   \n",
      "32  https://yt3.ggpht.com/3j-SmtLOCCkqTSWG0_J3X3jg...   \n",
      "33  https://yt3.ggpht.com/ytc/AIdro_kHVy2gbcw7kvNy...   \n",
      "34  https://yt3.ggpht.com/ytc/AIdro_k3nZezaA3EkCI6...   \n",
      "35  https://yt3.ggpht.com/ytc/AIdro_kcW9Pd82YM6NZ1...   \n",
      "36  https://yt3.ggpht.com/ytc/AIdro_m6vou30rw3auvR...   \n",
      "37  https://yt3.ggpht.com/ytc/AIdro_nxd9iVpXujEywI...   \n",
      "38  https://yt3.ggpht.com/ytc/AIdro_mBTnA6NL12SKNP...   \n",
      "39  https://yt3.ggpht.com/ytc/AIdro_loKD0hIVLfW_Su...   \n",
      "40  https://yt3.ggpht.com/ytc/AIdro_lWvSVSJ81sssmX...   \n",
      "41  https://yt3.ggpht.com/ytc/AIdro_n5jhBMgdrE7WFn...   \n",
      "42  https://yt3.ggpht.com/ytc/AIdro_nlnCbifs8Qgiov...   \n",
      "43  https://yt3.ggpht.com/ytc/AIdro_kHIJA-uiun8dLQ...   \n",
      "44  https://yt3.ggpht.com/ytc/AIdro_m-ONAstjj3BjNA...   \n",
      "45  https://yt3.ggpht.com/n1uidoXzHPD_WYYfVD0mcgsB...   \n",
      "46  https://yt3.ggpht.com/ytc/AIdro_kiUqyZ1XlMemoW...   \n",
      "47  https://yt3.ggpht.com/6UuVJBf-lC0MvVD_PZq9Krj_...   \n",
      "48  https://yt3.ggpht.com/ytc/AIdro_l0pUapmerya32E...   \n",
      "49  https://yt3.ggpht.com/kUGKbL8PDzFVuWaboKbvVn6x...   \n",
      "\n",
      "                                    authorChannelUrl  \\\n",
      "0               http://www.youtube.com/@LeilaGharani   \n",
      "1                   http://www.youtube.com/@Klarkooi   \n",
      "2              http://www.youtube.com/@adarshjha4774   \n",
      "3                  http://www.youtube.com/@truth5119   \n",
      "4             http://www.youtube.com/@michaeljarcher   \n",
      "5             http://www.youtube.com/@onewhitewolf13   \n",
      "6   http://www.youtube.com/@md.mahmudulalamsymon1216   \n",
      "7                 http://www.youtube.com/@iditenahui   \n",
      "8                 http://www.youtube.com/@siryoneyal   \n",
      "9                    http://www.youtube.com/@kn4golf   \n",
      "10               http://www.youtube.com/@walterf6763   \n",
      "11         http://www.youtube.com/@FutureCommentary1   \n",
      "12                   http://www.youtube.com/@m_stedt   \n",
      "13                    http://www.youtube.com/@wr3661   \n",
      "14          http://www.youtube.com/@SilverVillacacan   \n",
      "15                 http://www.youtube.com/@RayMedina   \n",
      "16                http://www.youtube.com/@gui.liraaa   \n",
      "17              http://www.youtube.com/@2000sunsunny   \n",
      "18         http://www.youtube.com/@prajwalshetty2047   \n",
      "19               http://www.youtube.com/@drewsoffice   \n",
      "20   http://www.youtube.com/@kashviyashvikimasti5389   \n",
      "21                 http://www.youtube.com/@dougc1753   \n",
      "22              http://www.youtube.com/@jesus.moreno   \n",
      "23                http://www.youtube.com/@TurnToData   \n",
      "24         http://www.youtube.com/@PoojaAdnani-wk6pp   \n",
      "25         http://www.youtube.com/@sarmadpirzada8485   \n",
      "26         http://www.youtube.com/@hamphrey.olendo65   \n",
      "27      http://www.youtube.com/@tanweerabbasabbas649   \n",
      "28           http://www.youtube.com/@user-xi8oh4ne4u   \n",
      "29           http://www.youtube.com/@fidahussain6430   \n",
      "30                 http://www.youtube.com/@mckavelli   \n",
      "31            http://www.youtube.com/@tamerhegab1513   \n",
      "32          http://www.youtube.com/@dianechapman9887   \n",
      "33               http://www.youtube.com/@arsakib8186   \n",
      "34           http://www.youtube.com/@joaquimcosta952   \n",
      "35          http://www.youtube.com/@ericssontest2332   \n",
      "36                  http://www.youtube.com/@AJ-et3vf   \n",
      "37            http://www.youtube.com/@johnwilson7845   \n",
      "38           http://www.youtube.com/@hadiasghari2297   \n",
      "39           http://www.youtube.com/@omidshirazi4166   \n",
      "40               http://www.youtube.com/@yachid_6516   \n",
      "41           http://www.youtube.com/@enocharthur4322   \n",
      "42            http://www.youtube.com/@FarhanMerchant   \n",
      "43           http://www.youtube.com/@JakeSmith-jy1kx   \n",
      "44                 http://www.youtube.com/@garys2187   \n",
      "45                 http://www.youtube.com/@Moker2012   \n",
      "46                http://www.youtube.com/@mrjaydp123   \n",
      "47           http://www.youtube.com/@user-gm5dc4mt6t   \n",
      "48                 http://www.youtube.com/@Zoe-dm1ch   \n",
      "49           http://www.youtube.com/@user-wx1ti7qo5z   \n",
      "\n",
      "                          authorChannelId  canRate viewerRating  likeCount  \\\n",
      "0   {'value': 'UCJtUOos_MwJa_Ewii-R3cJA'}     True         none          0   \n",
      "1   {'value': 'UCh2TWLCWl6AWoaqZRVPRysA'}     True         none          0   \n",
      "2   {'value': 'UCJ2Ztp58rmIHrjnhfjNtEgQ'}     True         none          0   \n",
      "3   {'value': 'UCXnW-bcZXAdS17NPeiwaLEg'}     True         none          0   \n",
      "4   {'value': 'UChtwDOSItj9dZWgizHMlCmw'}     True         none          0   \n",
      "5   {'value': 'UChxPxoKzQvhpzSQlbPcZMuw'}     True         none          1   \n",
      "6   {'value': 'UC4dmViNBRSMYUMPV25TEiLQ'}     True         none          0   \n",
      "7   {'value': 'UCU-O-6mg3rPozMkk8J-3mbg'}     True         none          0   \n",
      "8   {'value': 'UCZAii0P__sOASNXeoJFkNRQ'}     True         none          0   \n",
      "9   {'value': 'UCjjTx8FontwigezjNj37q1w'}     True         none          0   \n",
      "10  {'value': 'UCJxqADSWk_LuBMWjcN55MtA'}     True         none          0   \n",
      "11  {'value': 'UCu4nkB4Fdcvmt5s0oG3rWEw'}     True         none          1   \n",
      "12  {'value': 'UC9_C465HlPEniAoXEhJI3qw'}     True         none          0   \n",
      "13  {'value': 'UCrraIlTaMSwqpEJcRzhUzGQ'}     True         none          0   \n",
      "14  {'value': 'UCVClvnotp6YjxBXjk-FOisQ'}     True         none          0   \n",
      "15  {'value': 'UCTQXCTvyYN7_fv4gg6rAAmQ'}     True         none          0   \n",
      "16  {'value': 'UCEIDbNmS_iotKIjZtfkPZnA'}     True         none          0   \n",
      "17  {'value': 'UCgp9tzzSy2KNpBfkMhweZYg'}     True         none          0   \n",
      "18  {'value': 'UCMfn9Ixqllf6nqkCwu_9ECA'}     True         none          0   \n",
      "19  {'value': 'UCyDoSGJgy_maqQr_HqHWNww'}     True         none          0   \n",
      "20  {'value': 'UCqPhBt9ADKHbaCrGfXgCYkw'}     True         none          0   \n",
      "21  {'value': 'UCe8TNeEL8m6z5hhkFvGdyEw'}     True         none          0   \n",
      "22  {'value': 'UCT7GHpHMUKJRFsYlECORatw'}     True         none          0   \n",
      "23  {'value': 'UC46QfoiHvwmFchMlD-zhWyQ'}     True         none          1   \n",
      "24  {'value': 'UC4ZMu01PEIVdPVrwPZ8tIdg'}     True         none          0   \n",
      "25  {'value': 'UCIftXK3-KhKY9i984LhiGIg'}     True         none          0   \n",
      "26  {'value': 'UCDURT1uVxSJNXp4iX7T52aA'}     True         none          0   \n",
      "27  {'value': 'UC9W2hcRDZy27sDu4DVJdbvg'}     True         none          0   \n",
      "28  {'value': 'UCSCEgFsHMuattKkVee6PS-g'}     True         none          0   \n",
      "29  {'value': 'UCO5a0Jxp8kBlSdo3aOPlT7w'}     True         none          0   \n",
      "30  {'value': 'UC6iVt2bYb03FuFr_nPiJCNA'}     True         none          0   \n",
      "31  {'value': 'UC4-vOP2jj0oPJ9EvDRIybTg'}     True         none          0   \n",
      "32  {'value': 'UCDnepmj50nntg9V-dk4nBtg'}     True         none          0   \n",
      "33  {'value': 'UCV4JfSeln8q_MplmvRU-azQ'}     True         none          0   \n",
      "34  {'value': 'UCHfTHD2HlP-7U_zDeiI7lOQ'}     True         none          0   \n",
      "35  {'value': 'UCMMIk4_f1YDr85jyG1Whr_A'}     True         none          0   \n",
      "36  {'value': 'UCYTb79L_IlA_7Jd_tjlPTIQ'}     True         none          0   \n",
      "37  {'value': 'UCU7sHlZXEsa9BGdYP4vi7zg'}     True         none          3   \n",
      "38  {'value': 'UCirOumflF2GjuTsGjVCNRXw'}     True         none          0   \n",
      "39  {'value': 'UCcOugVU-cIDqsQdERyw8WMg'}     True         none          0   \n",
      "40  {'value': 'UC5eVcyf7nncXGaIBY_nlxOQ'}     True         none          1   \n",
      "41  {'value': 'UCE6C1aRHpfaxOdbNH11gRrA'}     True         none          0   \n",
      "42  {'value': 'UChqVfvF7EV3p-_i_ZKeJRGw'}     True         none          0   \n",
      "43  {'value': 'UCSCK2yF1O8z3LkfKdZlcX2w'}     True         none          0   \n",
      "44  {'value': 'UCVV4Ry2j6EKmwtgSevSZbHg'}     True         none          0   \n",
      "45  {'value': 'UCQbMGZS88pyJtSvZi1AtX2g'}     True         none          0   \n",
      "46  {'value': 'UCkw32L6Ydi4fCBW_TlcnoYg'}     True         none          0   \n",
      "47  {'value': 'UCI-3ic-vE3cQrSdLwbnyuqQ'}     True         none          2   \n",
      "48  {'value': 'UC8kj5-_Iq2zXW7t-VkSId8w'}     True         none          0   \n",
      "49  {'value': 'UCGfTseOViTiyhcG02Nq50Lg'}     True         none          0   \n",
      "\n",
      "             publishedAt             updatedAt  \n",
      "0   2024-01-25T17:16:47Z  2024-01-25T17:16:47Z  \n",
      "1   2024-04-14T12:05:55Z  2024-04-14T12:05:55Z  \n",
      "2   2024-02-13T16:06:47Z  2024-02-13T16:06:47Z  \n",
      "3   2023-12-29T07:35:07Z  2023-12-29T07:35:21Z  \n",
      "4   2023-11-24T12:17:39Z  2023-11-24T12:17:39Z  \n",
      "5   2023-11-01T06:58:12Z  2023-11-01T06:58:12Z  \n",
      "6   2023-10-18T18:12:05Z  2023-10-18T18:12:05Z  \n",
      "7   2023-09-23T18:50:10Z  2023-09-23T18:50:10Z  \n",
      "8   2023-09-06T09:13:18Z  2023-09-06T09:13:18Z  \n",
      "9   2023-09-05T14:03:32Z  2023-09-05T14:03:32Z  \n",
      "10  2023-09-04T21:57:03Z  2023-09-04T21:57:03Z  \n",
      "11  2023-08-31T05:58:46Z  2023-08-31T05:58:45Z  \n",
      "12  2023-08-28T11:24:58Z  2023-08-28T11:24:57Z  \n",
      "13  2023-08-27T23:44:04Z  2023-08-27T23:44:03Z  \n",
      "14  2023-08-25T10:33:41Z  2023-08-25T10:33:41Z  \n",
      "15  2023-08-22T00:45:58Z  2023-08-22T00:45:57Z  \n",
      "16  2023-08-18T20:45:41Z  2023-08-18T20:45:41Z  \n",
      "17  2023-08-14T10:38:16Z  2023-08-14T10:38:15Z  \n",
      "18  2023-08-13T14:43:40Z  2023-08-13T14:43:40Z  \n",
      "19  2023-07-27T20:45:29Z  2023-07-27T20:45:29Z  \n",
      "20  2023-07-23T09:08:38Z  2023-07-23T09:08:38Z  \n",
      "21  2023-07-20T20:36:30Z  2023-07-20T20:36:30Z  \n",
      "22  2023-07-20T03:39:33Z  2023-07-20T03:39:33Z  \n",
      "23  2023-07-19T09:00:42Z  2023-07-19T09:00:42Z  \n",
      "24  2023-07-17T06:25:31Z  2023-07-17T06:25:31Z  \n",
      "25  2023-07-15T04:49:19Z  2023-07-15T04:49:19Z  \n",
      "26  2023-07-14T20:55:52Z  2023-07-14T20:55:52Z  \n",
      "27  2023-07-12T18:07:17Z  2023-07-12T18:07:17Z  \n",
      "28  2023-07-12T06:42:06Z  2023-07-12T06:42:06Z  \n",
      "29  2023-07-11T22:35:47Z  2023-07-11T22:35:47Z  \n",
      "30  2023-07-11T22:11:17Z  2023-07-11T22:11:17Z  \n",
      "31  2023-07-11T22:06:28Z  2023-07-11T22:06:28Z  \n",
      "32  2023-07-11T14:55:25Z  2023-07-11T14:55:25Z  \n",
      "33  2023-07-11T13:58:09Z  2023-07-11T13:58:09Z  \n",
      "34  2023-07-11T01:34:19Z  2023-07-11T01:34:19Z  \n",
      "35  2023-07-10T21:38:20Z  2023-07-10T21:38:20Z  \n",
      "36  2023-07-10T12:40:39Z  2023-07-10T12:40:39Z  \n",
      "37  2023-07-10T12:24:20Z  2023-07-10T12:24:20Z  \n",
      "38  2023-07-10T02:44:43Z  2023-07-10T02:44:43Z  \n",
      "39  2023-07-09T23:13:36Z  2023-07-09T23:13:36Z  \n",
      "40  2023-07-09T16:56:39Z  2023-07-09T16:56:39Z  \n",
      "41  2023-07-09T11:18:36Z  2023-07-09T11:18:36Z  \n",
      "42  2023-07-09T10:13:32Z  2023-07-09T10:13:32Z  \n",
      "43  2023-07-09T01:24:36Z  2023-07-09T01:24:36Z  \n",
      "44  2023-07-08T19:56:56Z  2023-07-08T19:56:56Z  \n",
      "45  2023-07-08T19:31:12Z  2023-07-08T19:31:12Z  \n",
      "46  2023-07-08T19:20:31Z  2023-07-08T19:20:31Z  \n",
      "47  2023-07-08T15:15:14Z  2023-07-08T15:15:14Z  \n",
      "48  2023-07-08T10:27:55Z  2023-07-08T10:27:55Z  \n",
      "49  2023-07-08T09:30:06Z  2023-07-08T09:30:06Z  \n"
     ]
    }
   ],
   "source": [
    "import googleapiclient.discovery\n",
    "import pandas as pd\n",
    "\n",
    "api_service_name = \"youtube\"\n",
    "api_version = \"v3\"\n",
    "DEVELOPER_KEY = \"AIzaSyD1lXAVHpBQbDXaao5C-kTrBBkDbn1tvEI\"\n",
    "\n",
    "youtube = googleapiclient.discovery.build(\n",
    "    api_service_name, api_version, developerKey=DEVELOPER_KEY)\n",
    "\n",
    "request = youtube.commentThreads().list(\n",
    "    part=\"snippet\",\n",
    "    videoId=\"WNrB1Q9Rry0\",\n",
    "    maxResults=50\n",
    ")\n",
    "\n",
    "response = request.execute()\n",
    "\n",
    "comments = []\n",
    "\n",
    "for item in response['items']:\n",
    "    comment = item['snippet']['topLevelComment']['snippet']\n",
    "    comments.append(comment)\n",
    "\n",
    "df = pd.DataFrame(comments)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Comment\n",
      "0                          1 tb variant nhee hai kya\n",
      "1               Performance processor ko bhi sudharo\n",
      "2  Vivo v30 pro vs Motorola edge 50 pro compariso...\n",
      "3                              Battery los  Motorola\n",
      "4                                  call recording he\n",
      "5                   Samsung A55 se compare karna hai\n",
      "6              Sir please compare with one plus 12 r\n",
      "7   Video sab dekh te hai par koi leta nahi hai üòÖüòÖüòÖüòÖ\n",
      "8  I have moto edge 40 ,I never got any major upd...\n",
      "9                          Redmi note 13 pro plus 5g\n"
     ]
    }
   ],
   "source": [
    "import googleapiclient.discovery\n",
    "import pandas as pd\n",
    "\n",
    "api_service_name = \"youtube\"\n",
    "api_version = \"v3\"\n",
    "DEVELOPER_KEY = \"AIzaSyD1lXAVHpBQbDXaao5C-kTrBBkDbn1tvEI\"\n",
    "\n",
    "youtube = googleapiclient.discovery.build(\n",
    "    api_service_name, api_version, developerKey=DEVELOPER_KEY)\n",
    "\n",
    "request = youtube.commentThreads().list(\n",
    "    part=\"snippet\",\n",
    "    videoId=\"ij9AcZwMf2I\",\n",
    "    maxResults=10\n",
    ")\n",
    "\n",
    "response = request.execute()\n",
    "\n",
    "comments = []\n",
    "\n",
    "for item in response['items']:\n",
    "    comment_text = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "    comments.append(comment_text)\n",
    "\n",
    "df = pd.DataFrame(comments, columns=['Comment'])\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Comment\n",
      "0                          1 tb variant nhee hai kya\n",
      "1               Performance processor ko bhi sudharo\n",
      "2  Vivo v30 pro vs Motorola edge 50 pro compariso...\n",
      "3                              Battery los  Motorola\n",
      "4                                  call recording he\n",
      "5                   Samsung A55 se compare karna hai\n",
      "6              Sir please compare with one plus 12 r\n",
      "7   Video sab dekh te hai par koi leta nahi hai üòÖüòÖüòÖüòÖ\n",
      "8  I have moto edge 40 ,I never got any major upd...\n",
      "9                          Redmi note 13 pro plus 5g\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 tb variant nhee hai kya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Performance processor ko bhi sudharo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vivo v30 pro vs Motorola edge 50 pro compariso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Battery los  Motorola</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>call recording he</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Samsung A55 se compare karna hai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sir please compare with one plus 12 r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Video sab dekh te hai par koi leta nahi hai üòÖüòÖüòÖüòÖ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I have moto edge 40 ,I never got any major upd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Redmi note 13 pro plus 5g</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comment\n",
       "0                          1 tb variant nhee hai kya\n",
       "1               Performance processor ko bhi sudharo\n",
       "2  Vivo v30 pro vs Motorola edge 50 pro compariso...\n",
       "3                              Battery los  Motorola\n",
       "4                                  call recording he\n",
       "5                   Samsung A55 se compare karna hai\n",
       "6              Sir please compare with one plus 12 r\n",
       "7   Video sab dekh te hai par koi leta nahi hai üòÖüòÖüòÖüòÖ\n",
       "8  I have moto edge 40 ,I never got any major upd...\n",
       "9                          Redmi note 13 pro plus 5g"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Set display options to show all rows and columns\n",
    "pd.set_option('display.max_rows', None)  # Set to None to display all rows\n",
    "pd.set_option('display.max_columns', None)  # Set to None to display all columns\n",
    "pd.set_option('display.width', None)  # Set to None to auto-detect display width\n",
    "\n",
    "# Your DataFrame (df) here\n",
    "print(df)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('C:\\\\Users\\\\SATWIK M BADIGER\\\\Desktop\\\\projects\\\\ML\\\\Sentimeter\\\\comments.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 40\u001b[0m\n\u001b[0;32m     35\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPreprocessed_Comment\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(tokenize_comment)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Assuming you have 'vectorizer' available, if not, you need to load it from a file as well\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Vectorization function (using the same vectorizer as used during training)\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m X_comments_bow \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241m.\u001b[39mtransform([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(tokens) \u001b[38;5;28;01mfor\u001b[39;00m tokens \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTokens\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Predict sentiment using the trained model\u001b[39;00m\n\u001b[0;32m     43\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentiment\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(X_comments_bow)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Load the trained model from the file\n",
    "with open(\"C:\\\\Users\\\\SATWIK M BADIGER\\\\Desktop\\\\projects\\\\ML\\\\Sentimeter\\\\naive_bayes_model.pkl\", 'rb') as file:\n",
    "    clf = pickle.load(file)\n",
    "\n",
    "# Sample comments for demonstration\n",
    "comments = [\n",
    "    \"This product is amazing!\",\n",
    "    \"I'm not satisfied with the service.\",\n",
    "    \"The movie was disappointing.\",\n",
    "    \"The food was delicious.\"\n",
    "]\n",
    "\n",
    "# Create a DataFrame with the comments\n",
    "df = pd.DataFrame(comments, columns=['Comment'])\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_comment(comment):\n",
    "    # Lowercase each comment and remove non-alphabetic characters\n",
    "    comment = re.sub(r'[^a-zA-Z\\s]', '', comment).lower()\n",
    "    return comment\n",
    "\n",
    "# Apply preprocessing to all comments in the DataFrame\n",
    "df['Preprocessed_Comment'] = df['Comment'].apply(preprocess_comment)\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_comment(comment):\n",
    "    return word_tokenize(comment)\n",
    "\n",
    "# Tokenize the preprocessed comments\n",
    "df['Tokens'] = df['Preprocessed_Comment'].apply(tokenize_comment)\n",
    "\n",
    "# Assuming you have 'vectorizer' available, if not, you need to load it from a file as well\n",
    "\n",
    "# Vectorization function (using the same vectorizer as used during training)\n",
    "X_comments_bow = vectorizer.transform([' '.join(tokens) for tokens in df['Tokens']])\n",
    "\n",
    "# Predict sentiment using the trained model\n",
    "df['Sentiment'] = clf.predict(X_comments_bow)\n",
    "\n",
    "# Display the DataFrame with sentiment predictions\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8874743421204364\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "csv_file_path = \"C:\\\\Users\\\\SATWIK M BADIGER\\\\Desktop\\\\projects\\\\ML\\\\Sentimeter\\\\dataset.csv\"\n",
    "\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "df = df[df['Language'] == 'en']\n",
    "\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "df['Text'] = df['Text'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x))\n",
    "\n",
    "df['Text'] = df['Text'].apply(word_tokenize)\n",
    "\n",
    "# Vectorization\n",
    "vectorizer = CountVectorizer()\n",
    "X_bow = vectorizer.fit_transform([' '.join(tokens) for tokens in df['Text']])\n",
    "\n",
    "#test and train data splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bow, df['Label'], test_size=0.2, random_state=42)\n",
    "\n",
    "#training Naive Bayes classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#making predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "#model accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Comment\n",
      "0  Your words become things. Manifest wisely:<br>...\n",
      "1                Oprah and the money preacher?? Nope\n",
      "2  Really need help with my mind set it really ke...\n",
      "3  Who is the last person that appears on the video?\n",
      "4                                     Get lost Harpo\n",
      "5                                  Clear mind spaceüôè\n",
      "6                                          Lol oprah\n",
      "7                  Thank you so much for this üôåüèæüôåüèæüôåüèæ\n",
      "8                      Minestroney Soup Acne Face. ü§Æ\n",
      "9          i want to sleep forever and never wake up\n"
     ]
    }
   ],
   "source": [
    "import googleapiclient.discovery\n",
    "import pandas as pd\n",
    "\n",
    "api_service_name = \"youtube\"\n",
    "api_version = \"v3\"\n",
    "DEVELOPER_KEY = \"AIzaSyD1lXAVHpBQbDXaao5C-kTrBBkDbn1tvEI\"\n",
    "\n",
    "youtube = googleapiclient.discovery.build(\n",
    "    api_service_name, api_version, developerKey=DEVELOPER_KEY)\n",
    "\n",
    "request = youtube.commentThreads().list(\n",
    "    part=\"snippet\",\n",
    "    videoId=\"u53NWioihto\",\n",
    "    maxResults=10\n",
    ")\n",
    "\n",
    "response = request.execute()\n",
    "\n",
    "comments = []\n",
    "\n",
    "for item in response['items']:\n",
    "    comment_text = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "    comments.append(comment_text)\n",
    "\n",
    "df = pd.DataFrame(comments, columns=['Comment'])\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n",
      "6 3 0 0 0\n"
     ]
    }
   ],
   "source": [
    "def preprocess_comment(comment):\n",
    "    # Lowercase each comment and remove non-alphabetic characters\n",
    "    comment = re.sub(r'[^a-zA-Z\\s]', '', comment).lower()\n",
    "    return comment\n",
    "\n",
    "# Apply preprocessing to all comments in the DataFrame\n",
    "df['Preprocessed_Comment'] = df['Comment'].apply(preprocess_comment)\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_comment(comment):\n",
    "    return word_tokenize(comment)\n",
    "\n",
    "# Tokenize the preprocessed comments\n",
    "df['Tokens'] = df['Preprocessed_Comment'].apply(tokenize_comment)\n",
    "\n",
    "# Vectorization function (using the same vectorizer as used during training)\n",
    "# Assuming you have 'vectorizer' available, if not, you need to load it from a file as well\n",
    "X_comments_bow = vectorizer.transform([' '.join(tokens) for tokens in df['Tokens']])\n",
    "\n",
    "# Predict sentiment for each comment\n",
    "df['Sentiment'] = clf.predict(X_comments_bow)\n",
    "\n",
    "# Count the occurrences of each sentiment label\n",
    "sentiment_counts = df['Sentiment'].value_counts()\n",
    "\n",
    "# Get counts for each sentiment label, defaulting to 0 if not found\n",
    "positive_count = sentiment_counts.get('positive', 0)\n",
    "negative_count = sentiment_counts.get('negative', 0)\n",
    "uncertain_count = sentiment_counts.get('uncertainy', 0)\n",
    "litigious_count = sentiment_counts.get('litigious', 0)\n",
    "if(positive_count == negative_count):\n",
    "    neutral_count = positive_count\n",
    "else:\n",
    "    neutral_count = 0\n",
    "\n",
    "# Check the counts to determine the overall sentiment\n",
    "if positive_count > negative_count and positive_count > uncertain_count and positive_count > litigious_count:\n",
    "    print(\"Positive\")\n",
    "elif negative_count > positive_count and negative_count > uncertain_count and negative_count > litigious_count:\n",
    "    print(\"Negative\")\n",
    "elif uncertain_count > positive_count and uncertain_count > negative_count and uncertain_count > litigious_count:\n",
    "    print(\"Uncertain\")\n",
    "elif litigious_count > positive_count and litigious_count > negative_count and litigious_count > uncertain_count:\n",
    "    print(\"Litigious\")\n",
    "elif positive_count == negative_count:\n",
    "    print(\"Neutral\")\n",
    "else:\n",
    "    print(\"Uncertain\")\n",
    "\n",
    "print(positive_count, negative_count, uncertain_count, litigious_count, neutral_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8874743421204364\n",
      "                                             Comment\n",
      "0  Moto 2 saal ke Android aur back me.gls nhi hai...\n",
      "1                          1 tb variant nhee hai kya\n",
      "2               Performance processor ko bhi sudharo\n",
      "3  Vivo v30 pro vs Motorola edge 50 pro compariso...\n",
      "4                              Battery los  Motorola\n",
      "5                                  call recording he\n",
      "6                   Samsung A55 se compare karna hai\n",
      "7              Sir please compare with one plus 12 r\n",
      "8   Video sab dekh te hai par koi leta nahi hai üòÖüòÖüòÖüòÖ\n",
      "9  I have moto edge 40 ,I never got any major upd...\n",
      "Litigious\n",
      "2 2 0 3 2\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from model import vectorizer, vector, clf\n",
    "from webscrapping import get_comments\n",
    "df = get_comments()\n",
    "\n",
    "def preprocess_comment(comment):\n",
    "    # Lowercase each comment and remove non-alphabetic characters\n",
    "    comment = re.sub(r'[^a-zA-Z\\s]', '', comment).lower()\n",
    "    return comment\n",
    "\n",
    "# Apply preprocessing to all comments in the DataFrame\n",
    "df['Preprocessed_Comment'] = df['Comment'].apply(preprocess_comment)\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_comment(comment):\n",
    "    return word_tokenize(comment)\n",
    "\n",
    "# Tokenize the preprocessed comments\n",
    "df['Tokens'] = df['Preprocessed_Comment'].apply(tokenize_comment)\n",
    "\n",
    "# Vectorization function (using the same vectorizer as used during training)\n",
    "# Assuming you have 'vectorizer' available, if not, you need to load it from a file as well\n",
    "X_comments_bow = vectorizer.transform([' '.join(tokens) for tokens in df['Tokens']])\n",
    "\n",
    "# Predict sentiment for each comment\n",
    "df['Sentiment'] = clf.predict(X_comments_bow)\n",
    "\n",
    "# Count the occurrences of each sentiment label\n",
    "sentiment_counts = df['Sentiment'].value_counts()\n",
    "\n",
    "# Get counts for each sentiment label, defaulting to 0 if not found\n",
    "positive_count = sentiment_counts.get('positive', 0)\n",
    "negative_count = sentiment_counts.get('negative', 0)\n",
    "uncertain_count = sentiment_counts.get('uncertainy', 0)\n",
    "litigious_count = sentiment_counts.get('litigious', 0)\n",
    "if(positive_count == negative_count):\n",
    "    neutral_count = positive_count\n",
    "else:\n",
    "    neutral_count = 0\n",
    "\n",
    "# Check the counts to determine the overall sentiment\n",
    "if positive_count > negative_count and positive_count > uncertain_count and positive_count > litigious_count:\n",
    "    print(\"Positive\")\n",
    "elif negative_count > positive_count and negative_count > uncertain_count and negative_count > litigious_count:\n",
    "    print(\"Negative\")\n",
    "elif uncertain_count > positive_count and uncertain_count > negative_count and uncertain_count > litigious_count:\n",
    "    print(\"Uncertain\")\n",
    "elif litigious_count > positive_count and litigious_count > negative_count and litigious_count > uncertain_count:\n",
    "    print(\"Litigious\")\n",
    "elif positive_count == negative_count:\n",
    "    print(\"Neutral\")\n",
    "else:\n",
    "    print(\"Uncertain\")\n",
    "\n",
    "print(positive_count, negative_count, uncertain_count, litigious_count, neutral_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
